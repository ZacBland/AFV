{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.41s)\n",
      "creating index...\n",
      "index created!\n",
      "10742\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.14s)\n",
      "creating index...\n",
      "index created!\n",
      "Start Training\n",
      "Train Loss: 0.8508176141025322\n",
      "Accuracy: 2.2744755244755246\n",
      "tensor(1.0523, device='cuda:0', grad_fn=<AddBackward0>) inf\n",
      "Improvement.\n",
      "Train Loss: 0.8208928845839454\n",
      "Accuracy: 2.5786713286713288\n",
      "tensor(0.7991, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.0523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Improvement.\n",
      "Train Loss: 0.8100025210243859\n",
      "Accuracy: 2.7263986013986012\n",
      "tensor(0.8439, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.7991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train Loss: 0.7977807238720197\n",
      "Accuracy: 2.88986013986014\n",
      "tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.7991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Improvement.\n",
      "Train Loss: 0.7883491154013955\n",
      "Accuracy: 3.050699300699301\n",
      "tensor(1.0992, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train Loss: 0.7778357441966337\n",
      "Accuracy: 2.9239510489510487\n",
      "tensor(0.5586, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train Loss: 0.7700250740658855\n",
      "Accuracy: 2.925699300699301\n",
      "tensor(0.8212, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train Loss: 0.7616698858969618\n",
      "Accuracy: 3.027097902097902\n",
      "tensor(0.7063, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train Loss: 0.7550526040863476\n",
      "Accuracy: 3.0\n",
      "tensor(1.0149, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train Loss: 0.7476536455388755\n",
      "Accuracy: 3.01486013986014\n",
      "tensor(1.0201, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train Loss: 0.7417564792967695\n",
      "Accuracy: 3.104020979020979\n",
      "tensor(1.0317, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train Loss: 0.7355639189148873\n",
      "Accuracy: 2.812062937062937\n",
      "tensor(0.6316, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train Loss: 0.7298752171884076\n",
      "Accuracy: 3.007867132867133\n",
      "tensor(0.6956, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Train Loss: 0.724552723137372\n",
      "Accuracy: 3.125874125874126\n",
      "tensor(1.0265, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Early stopping!\n",
      "Start Testing\n",
      "Accuracy: 0.0019456503255167973\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
    "from torchvision.datasets import CocoDetection\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Load the pre-trained model\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT).to(device)\n",
    "\n",
    "# Freeze the parameters of the base network\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define the transform\n",
    "transform = T.Compose([\n",
    "    T.Resize((640, 512)),  # Resize all images to have size 800x800\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Load the training data\n",
    "train_data = CocoDetection(root='./FLIR_ADAS_v2/images_thermal_train', annFile='./FLIR_ADAS_v2/images_thermal_train/coco.json', transform=transform)\n",
    "# Load the validation and test data\n",
    "print(len(train_data))\n",
    "val_data = CocoDetection(root='./FLIR_ADAS_v2/images_thermal_val', annFile='./FLIR_ADAS_v2/images_thermal_val/coco.json', transform=transform)\n",
    "test_data = CocoDetection(root='./FLIR_ADAS_v2/video_thermal_test', annFile='./FLIR_ADAS_v2/video_thermal_test/coco.json', transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True,num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=4, shuffle=False,num_workers=4,collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=4, shuffle=False,num_workers=4,collate_fn=collate_fn)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch = 100\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "n_epochs_stop = 10\n",
    "print('Start Training')\n",
    "# Train the model\n",
    "for epoch in range(epoch):  # loop over the dataset multiple times\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, targets)\n",
    "        loss = sum(loss for loss in outputs.values())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    writer.add_scalar('Loss/train', running_loss/len(train_loader), epoch)\n",
    "    print(f'Train Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, targets = data\n",
    "            inputs = inputs.to(device)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(inputs)\n",
    "            for j, output in enumerate(outputs):\n",
    "                for pred_box, pred_label in zip(output['boxes'], output['labels']):\n",
    "                    for true_box, true_label in zip(targets[j]['boxes'], targets[j]['labels']):\n",
    "                        if pred_label == true_label and calculate_iou(pred_box, true_box) > 0.5:\n",
    "                            accuracy += 1\n",
    "\n",
    "                                                        \n",
    "\n",
    "\n",
    "                            \n",
    "    accuracy=accuracy / len(val_data)\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "\n",
    "    print(loss, best_val_loss)\n",
    "    if loss < best_val_loss:\n",
    "        print(\"Improvement.\")\n",
    "        best_val_loss = loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == n_epochs_stop:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "# Testing\n",
    "print('Start Testing')\n",
    "model.eval()\n",
    "running_loss = 0.0\n",
    "accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "            inputs, targets = data\n",
    "            inputs = inputs.to(device)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            for j, output in enumerate(outputs):\n",
    "                for pred_box, pred_label in zip(output['boxes'], output['labels']):\n",
    "                    for true_box, true_label in zip(targets[j]['boxes'], targets[j]['labels']):\n",
    "                        if pred_label == true_label and calculate_iou(pred_box, true_box) > 0.5:\n",
    "                            accuracy += 1\n",
    "                        \n",
    "\n",
    "accuracy /= len(test_data)\n",
    "print(f'Accuracy: {accuracy/len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model\n",
      "Model saved successfully at: model_save_folder/model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('Saving the model')\n",
    "# Create the model save folder\n",
    "save_folder = 'model_save_folder'\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model_save_path = os.path.join(save_folder, 'model.pth')\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print('Model saved successfully at:', model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing the model\n",
      "Quantized model saved successfully at: model_save_folder/quantized_model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch.quantization as quant\n",
    "print('Quantizing the model')\n",
    "# Quantize the model\n",
    "quantized_model = quant.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "# Save the quantized model\n",
    "quantized_model_save_path = os.path.join(save_folder, 'quantized_model.pth')\n",
    "torch.save(quantized_model.state_dict(), quantized_model_save_path)\n",
    "\n",
    "print('Quantized model saved successfully at:', quantized_model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[615.7820,  34.4937, 640.0000,  93.9286],\n",
      "        [231.7594,  86.6053, 587.2290, 373.6215],\n",
      "        [620.0148,  74.4984, 640.0000, 145.3932],\n",
      "        [615.9513,  34.1696, 639.8505,  94.1090],\n",
      "        [620.1577,  74.2401, 639.8654, 146.0039],\n",
      "        [599.5904, 383.7763, 640.0000, 438.9997],\n",
      "        [222.3247,  46.8173, 547.0507, 369.3022],\n",
      "        [603.6516, 391.9545, 638.0000, 417.4005],\n",
      "        [599.5909, 262.1013, 640.0000, 327.4353],\n",
      "        [593.4579, 208.0368, 621.3203, 257.0588],\n",
      "        [603.6488, 176.8571, 640.0000, 231.3294],\n",
      "        [576.1081,  24.7783, 640.0000,  83.5713],\n",
      "        [607.1165, 162.2090, 640.0000, 212.1892],\n",
      "        [595.8990, 133.9606, 638.1158, 194.7103],\n",
      "        [609.6840, 392.6681, 618.6058, 411.4550],\n",
      "        [612.4613, 286.5816, 618.5161, 297.9756],\n",
      "        [593.6612,  56.3283, 640.0000, 152.6759],\n",
      "        [612.4352, 267.2441, 618.4233, 278.5969],\n",
      "        [612.4224, 247.9309, 618.3334, 259.1940],\n",
      "        [612.4194, 228.6357, 618.2480, 239.7884],\n",
      "        [612.4208, 209.3475, 618.1866, 220.3993],\n",
      "        [604.1700, 378.2598, 624.8579, 431.2364]], device='cuda:0'), 'labels': tensor([12,  3, 12, 10, 10, 12,  1, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12], device='cuda:0'), 'scores': tensor([0.2002, 0.1516, 0.1222, 0.1073, 0.1008, 0.1004, 0.0985, 0.0837, 0.0768,\n",
      "        0.0679, 0.0654, 0.0598, 0.0596, 0.0541, 0.0532, 0.0521, 0.0518, 0.0517,\n",
      "        0.0514, 0.0511, 0.0507, 0.0501], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "quantized_model.eval()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load an image\n",
    "image = Image.open('FLIR0004.jpg')\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Convert the image to a PyTorch Tensor\n",
    "])\n",
    "\n",
    "# Move the image to the CPU\n",
    "image = image\n",
    "\n",
    "# Add the code block below the existing code\n",
    "# FILEPATH: /home/sjhjrol/Documents/Capstone Modifed/AFV/Flir/flir_training.ipynb\n",
    "image = transform(image).unsqueeze(0)  # Add an extra dimension for the batch size\n",
    "\n",
    "# Input the image into the model\n",
    "with torch.no_grad():\n",
    "    output = model(image.to(device))\n",
    "\n",
    "# Display the output\n",
    "print(output)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Convert the image back to OpenCV format\n",
    "image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Get the bounding boxes from the model's output\n",
    "boxes = output[0]['boxes'].cpu().numpy().astype(np.int32)\n",
    "\n",
    "# Draw the bounding boxes on the image\n",
    "for box in boxes:\n",
    "    cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow('Image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
